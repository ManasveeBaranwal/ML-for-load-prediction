{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1"
      ],
      "metadata": {
        "id": "ZrqwSigPbSLb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WBktbMqn-bhw"
      },
      "outputs": [],
      "source": [
        "# region_rf_shape_scaled.py - Region-wise Shape Forecasting using Random Forest\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error, r2_score\n",
        "\n",
        "# -------------------------------------------------------------\n",
        "# 1. File Paths\n",
        "# -------------------------------------------------------------\n",
        "TRAIN_XLSX = \"/content/Hourly_Load_Data_2018_2022_LeapFixed.xlsx\"\n",
        "TEST_XLSX = \"/content/India_2023_Hourly_Load_Data.xlsx\"\n",
        "ANNUAL_XLSX = \"/content/Regional_Peak___Energy__2018_2023_ &2030.xlsx\"\n",
        "\n",
        "# -------------------------------------------------------------\n",
        "# 2. Load Data\n",
        "# -------------------------------------------------------------\n",
        "def load_data(file, years, sheet_name=None):\n",
        "    xl = pd.ExcelFile(file)\n",
        "    dfs = []\n",
        "    for y in years:\n",
        "        try:\n",
        "            df = xl.parse(str(y))\n",
        "        except:\n",
        "            df = xl.parse(sheet_name or \"Sheet1\")\n",
        "        df[\"Year\"] = y\n",
        "        df[\"ts\"] = pd.date_range(f\"{y}-01-01 00:00\", periods=len(df), freq=\"h\")\n",
        "        dfs.append(df.set_index(\"ts\"))\n",
        "    return pd.concat(dfs)\n",
        "\n",
        "train_raw = load_data(TRAIN_XLSX, list(range(2018, 2023)))\n",
        "test_raw = load_data(TEST_XLSX, [2023], sheet_name=\"Sheet1\")\n",
        "\n",
        "region_cols = [\"Northern Region\", \"North Eastern Region\", \"Eastern Region\",\n",
        "               \"Southern Region\", \"Western Region\"]\n",
        "\n",
        "# -------------------------------------------------------------\n",
        "# 3. Load Annual Data\n",
        "# -------------------------------------------------------------\n",
        "ann = pd.read_excel(ANNUAL_XLSX, engine=\"openpyxl\")\n",
        "ann.columns = [c.strip().lower() for c in ann.columns]\n",
        "for c in ann.columns:\n",
        "    if \"peak\" in c: ann = ann.rename(columns={c: \"year_peak\"})\n",
        "    if \"energy\" in c: ann = ann.rename(columns={c: \"year_energy\"})\n",
        "ann = ann.rename(columns={\"year\": \"Year\"})\n",
        "ann[\"Year\"] = ann[\"Year\"].astype(int)\n",
        "ann = ann.drop_duplicates(subset=\"Year\").set_index(\"Year\")\n",
        "\n",
        "for df in [train_raw, test_raw]:\n",
        "    df[\"year_peak\"] = df[\"Year\"].map(ann[\"year_peak\"])\n",
        "    df[\"year_energy\"] = df[\"Year\"].map(ann[\"year_energy\"])\n",
        "\n",
        "# -------------------------------------------------------------\n",
        "# 4. Feature Engineering (shape-based, no lags)\n",
        "# -------------------------------------------------------------\n",
        "def make_features(df):\n",
        "    feats = pd.DataFrame(index=df.index)\n",
        "    feats[\"hour\"] = df.index.hour\n",
        "    feats[\"dow\"] = df.index.dayofweek\n",
        "    feats[\"month\"] = df.index.month\n",
        "    feats[\"wknd\"] = (feats[\"dow\"] >= 5).astype(int)\n",
        "    feats[\"year_peak\"] = df[\"year_peak\"]\n",
        "    feats[\"year_energy\"] = df[\"year_energy\"]\n",
        "    feats[\"target\"] = df[\"Load\"] / df[\"year_peak\"]  # Normalized shape\n",
        "    return feats.dropna()\n",
        "\n",
        "# -------------------------------------------------------------\n",
        "# 5. Train + Evaluate Shape Model with Scaling\n",
        "# -------------------------------------------------------------\n",
        "all_metrics = {}\n",
        "all_preds = {}\n",
        "\n",
        "for reg in region_cols:\n",
        "    print(f\"\\n=== Training Shape Model (RF): {reg} ===\")\n",
        "    train = train_raw.copy()\n",
        "    test = test_raw.copy()\n",
        "\n",
        "    train[\"Load\"] = train[reg]\n",
        "    test[\"Load\"] = test[reg]\n",
        "\n",
        "    feat_train = make_features(train)\n",
        "    feat_test = make_features(test)\n",
        "\n",
        "    X_tr, y_tr = feat_train.drop(\"target\", axis=1), feat_train[\"target\"]\n",
        "    X_te, y_te = feat_test.drop(\"target\", axis=1), feat_test[\"target\"]\n",
        "\n",
        "    model = RandomForestRegressor(\n",
        "        n_estimators=200,\n",
        "        max_depth=12,\n",
        "        min_samples_split=10,\n",
        "        max_features=\"sqrt\",\n",
        "        n_jobs=-1,\n",
        "        random_state=42\n",
        "    )\n",
        "    model.fit(X_tr, y_tr)\n",
        "    shape_hat = model.predict(X_te)\n",
        "\n",
        "    # Rescale using 2023 peak\n",
        "    peak2023 = test[\"year_peak\"].iloc[0]\n",
        "    y_hat = shape_hat * peak2023\n",
        "    y_true = test[\"Load\"].loc[X_te.index]  # aligned actuals\n",
        "\n",
        "    rmse = np.sqrt(mean_squared_error(y_true, y_hat))\n",
        "    mape = mean_absolute_percentage_error(y_true, y_hat) * 100\n",
        "    mbe = 100 * (y_hat - y_true).mean() / y_true.mean()\n",
        "    r2 = r2_score(y_true, y_hat)\n",
        "\n",
        "    all_metrics[reg] = {\"RMSE\": rmse, \"MAPE\": mape, \"MBE\": mbe, \"R2\": r2}\n",
        "    all_preds[reg] = pd.DataFrame({\"Actual\": y_true, \"Predicted\": y_hat})\n",
        "\n",
        "# -------------------------------------------------------------\n",
        "# 6. Results Summary\n",
        "# -------------------------------------------------------------\n",
        "metrics_df = pd.DataFrame(all_metrics).T\n",
        "print(\"\\n--- Shape-Scaled Region-wise Metrics (2023) ---\")\n",
        "print(metrics_df.round(3))\n",
        "metrics_df.to_csv(\"region_rf_shape_metrics_2023.csv\")\n",
        "\n",
        "# Save predictions\n",
        "for reg in region_cols:\n",
        "    all_preds[reg].to_csv(f\"{reg.replace(' ', '_')}_rf_shape_predictions_2023.csv\")\n",
        "\n",
        "print(\"\\nSaved all scaled-shape RF results and predictions.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2."
      ],
      "metadata": {
        "id": "cKGoygerbYah"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "wPD3HIyHh0py",
        "outputId": "ad072a84-1f1b-4a98-8ef2-385114b4a1d8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Training Multi-Output Model: Northern Region ===\n",
            "\n",
            "=== Training Multi-Output Model: North Eastern Region ===\n",
            "\n",
            "=== Training Multi-Output Model: Eastern Region ===\n",
            "\n",
            "=== Training Multi-Output Model: Southern Region ===\n",
            "\n",
            "=== Training Multi-Output Model: Western Region ===\n",
            "\n",
            "--- All Region-wise Multi-output Metrics (2023) ---\n",
            "                                  RMSE     R2\n",
            "Region               Target                  \n",
            "Northern Region      target   1212.503  0.988\n",
            "                     mean_24   535.950  0.997\n",
            "                     std_24    767.593  0.784\n",
            "                     fft_1       0.146  0.346\n",
            "                     fft_2       0.130  0.319\n",
            "                     fft_3       0.100 -0.139\n",
            "North Eastern Region target     61.995  0.982\n",
            "                     mean_24    19.467  0.997\n",
            "                     std_24     28.516  0.738\n",
            "                     fft_1       0.071  0.671\n",
            "                     fft_2       0.103  0.721\n",
            "                     fft_3       0.064  0.032\n",
            "Eastern Region       target    274.737  0.977\n",
            "                     mean_24   243.997  0.978\n",
            "                     std_24     93.048  0.586\n",
            "                     fft_1       0.122  0.282\n",
            "                     fft_2       0.135  0.275\n",
            "                     fft_3       0.080 -0.023\n",
            "Southern Region      target   1069.539  0.971\n",
            "                     mean_24   626.851  0.974\n",
            "                     std_24    664.833  0.710\n",
            "                     fft_1       0.038  0.524\n",
            "                     fft_2       0.070  0.531\n",
            "                     fft_3       0.064  0.071\n",
            "Western Region       target   1110.889  0.963\n",
            "                     mean_24   991.127  0.943\n",
            "                     std_24    630.573  0.882\n",
            "                     fft_1       0.146  0.104\n",
            "                     fft_2       0.204  0.048\n",
            "                     fft_3       0.075  0.015\n"
          ]
        }
      ],
      "source": [
        "# multi_output_xgb_regionwise.py - Predicting 2023 Load using Multi-Output XGBoost per Region\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.multioutput import MultiOutputRegressor\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from scipy.stats import skew, kurtosis\n",
        "from numpy.fft import rfft\n",
        "\n",
        "# ------------------------------------------------------------- FILE PATHS\n",
        "TRAIN_XLSX  = \"/content/Hourly_Load_Data_2018_2022_LeapFixed.xlsx\"\n",
        "TEST_XLSX   = \"/content/India_2023_Hourly_Load_Data.xlsx\"\n",
        "ANNUAL_XLSX = \"/content/Regional_Peak___Energy__2018_2023_ &2030.xlsx\"\n",
        "# ------------------------------------------------------------- UTILITIES\n",
        "def add_timestamp(df):\n",
        "    year = int(df[\"Year\"].iloc[0])\n",
        "    idx = pd.date_range(f\"{year}-01-01 00:00\", periods=len(df), freq=\"h\")\n",
        "    df.insert(0, \"ts\", idx)\n",
        "    return df.set_index(\"ts\")\n",
        "\n",
        "def fft_feature_array(x, n=5):\n",
        "    fft_vals = np.abs(rfft(x - np.mean(x)))[: n + 1]\n",
        "    fft_vals = fft_vals / (np.linalg.norm(fft_vals) + 1e-6)\n",
        "    return fft_vals[1 : n + 1]\n",
        "\n",
        "def make_features(df):\n",
        "    feats = pd.DataFrame(index=df.index)\n",
        "    for h in [1, 2, 3, 6, 12, 24, 48, 168]:\n",
        "        feats[f\"lag_{h}\"] = df[\"Load\"].shift(h)\n",
        "    feats[\"peak_yday\"]   = df[\"Load\"].rolling(24).max().shift(1)\n",
        "    feats[\"energy_yday\"] = df[\"Load\"].rolling(24).sum().shift(1)\n",
        "    feats[\"hour\"]  = df.index.hour\n",
        "    feats[\"dow\"]   = df.index.dayofweek\n",
        "    feats[\"month\"] = df.index.month\n",
        "    feats[\"wknd\"]  = (feats[\"dow\"] >= 5).astype(int)\n",
        "    feats[\"year_peak\"]   = df[\"year_peak\"]\n",
        "    feats[\"year_energy\"] = df[\"year_energy\"]\n",
        "    feats[\"mean_24\"] = df[\"Load\"].rolling(24).mean().shift(1)\n",
        "    feats[\"std_24\"]  = df[\"Load\"].rolling(24).std().shift(1)\n",
        "    for i in range(1, 4):\n",
        "        feats[f\"fft_{i}\"] = (\n",
        "            df[\"Load\"].rolling(24)\n",
        "            .apply(lambda x: fft_feature_array(x)[i - 1] if (~np.isnan(x)).sum() == 24 else np.nan, raw=True)\n",
        "            .shift(1)\n",
        "        )\n",
        "    feats[\"target\"] = df[\"Load\"]\n",
        "    return feats.dropna()\n",
        "\n",
        "# ------------------------------------------------------------- LOAD DATA\n",
        "train_xl  = pd.ExcelFile(TRAIN_XLSX)\n",
        "train_raw = pd.concat([train_xl.parse(str(y)).assign(Year=y) for y in range(2018, 2023)], ignore_index=True)\n",
        "train_raw = add_timestamp(train_raw)\n",
        "\n",
        "test_raw = pd.read_excel(TEST_XLSX, sheet_name=\"Sheet1\")\n",
        "test_raw[\"Year\"] = 2023\n",
        "test_raw = add_timestamp(test_raw)\n",
        "\n",
        "region_cols = [\"Northern Region\", \"North Eastern Region\", \"Eastern Region\", \"Southern Region\", \"Western Region\"]\n",
        "\n",
        "ann = pd.read_excel(ANNUAL_XLSX, engine=\"openpyxl\")\n",
        "ann.columns = [c.strip().lower() for c in ann.columns]\n",
        "ann = ann.rename(columns={\n",
        "    \"year\": \"Year\",\n",
        "    next(c for c in ann.columns if \"peak\" in c): \"year_peak\",\n",
        "    next(c for c in ann.columns if \"energy\" in c): \"year_energy\",\n",
        "})\n",
        "ann[\"Year\"] = ann[\"Year\"].astype(int)\n",
        "ann = ann.drop_duplicates(\"Year\").set_index(\"Year\")\n",
        "\n",
        "for df in (train_raw, test_raw):\n",
        "    df[\"year_peak\"]   = df[\"Year\"].map(ann[\"year_peak\"])\n",
        "    df[\"year_energy\"] = df[\"Year\"].map(ann[\"year_energy\"])\n",
        "\n",
        "# ------------------------------------------------------------- REGION-WISE TRAINING\n",
        "results = {}\n",
        "\n",
        "for reg in region_cols:\n",
        "    print(f\"\\n=== Training Multi-Output Model: {reg} ===\")\n",
        "    tr_df = train_raw.copy()\n",
        "    te_df = test_raw.copy()\n",
        "    tr_df[\"Load\"] = tr_df[reg]\n",
        "    te_df[\"Load\"] = te_df[reg]\n",
        "\n",
        "    feat_tr = make_features(tr_df)\n",
        "    feat_te = make_features(te_df)\n",
        "\n",
        "    X_tr = feat_tr.drop(columns=[\"target\", \"mean_24\", \"std_24\", \"fft_1\", \"fft_2\", \"fft_3\"])\n",
        "    Y_tr = feat_tr[[\"target\", \"mean_24\", \"std_24\", \"fft_1\", \"fft_2\", \"fft_3\"]]\n",
        "    X_te = feat_te.drop(columns=[\"target\", \"mean_24\", \"std_24\", \"fft_1\", \"fft_2\", \"fft_3\"])\n",
        "    Y_te = feat_te[[\"target\", \"mean_24\", \"std_24\", \"fft_1\", \"fft_2\", \"fft_3\"]]\n",
        "\n",
        "    model = MultiOutputRegressor(\n",
        "        XGBRegressor(\n",
        "            n_estimators=300,\n",
        "            max_depth=6,\n",
        "            learning_rate=0.05,\n",
        "            subsample=0.8,\n",
        "            colsample_bytree=0.8,\n",
        "            tree_method=\"hist\",\n",
        "            objective=\"reg:squarederror\",\n",
        "            random_state=42,\n",
        "        )\n",
        "    )\n",
        "\n",
        "    model.fit(X_tr.astype(np.float32), Y_tr)\n",
        "    Y_hat = pd.DataFrame(model.predict(X_te.astype(np.float32)), columns=Y_te.columns, index=Y_te.index)\n",
        "\n",
        "    result = {}\n",
        "    for col in Y_te.columns:\n",
        "        result[col] = {\n",
        "            \"RMSE\": np.sqrt(mean_squared_error(Y_te[col], Y_hat[col])),\n",
        "            \"R2\": r2_score(Y_te[col], Y_hat[col])\n",
        "        }\n",
        "\n",
        "    Y_hat.to_csv(f\"{reg.replace(' ', '_')}_multioutput_predictions_2023.csv\")\n",
        "    pd.DataFrame(result).T.to_csv(f\"{reg.replace(' ', '_')}_multioutput_metrics_2023.csv\")\n",
        "    results[reg] = result\n",
        "\n",
        "# ------------------------------------------------------------- OVERALL METRICS\n",
        "metrics_df = pd.concat({k: pd.DataFrame(v).T for k, v in results.items()}, names=[\"Region\", \"Target\"])\n",
        "print(\"\\n--- All Region-wise Multi-output Metrics (2023) ---\")\n",
        "print(metrics_df.round(3))\n",
        "metrics_df.to_csv(\"regionwise_multioutput_metrics_2023.csv\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3."
      ],
      "metadata": {
        "id": "k3b9Gb0qbqLJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 382
        },
        "id": "3Hh2T429yMkA",
        "outputId": "378dfe1b-fadc-4715-b5bd-f7ce7e4bc908"
      },
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/Regional_Peak___Energy__2018_2023_ &2030.xlsx'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4110462821.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# --------------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mannual_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/content/Regional_Peak___Energy__2018_2023_ &2030.xlsx\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mann_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_excel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mannual_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"openpyxl\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0mann_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mann_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36mread_excel\u001b[0;34m(io, sheet_name, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, parse_dates, date_parser, date_format, thousands, decimal, comment, skipfooter, storage_options, dtype_backend, engine_kwargs)\u001b[0m\n\u001b[1;32m    493\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExcelFile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m         \u001b[0mshould_close\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 495\u001b[0;31m         io = ExcelFile(\n\u001b[0m\u001b[1;32m    496\u001b[0m             \u001b[0mio\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    497\u001b[0m             \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, path_or_buffer, engine, storage_options, engine_kwargs)\u001b[0m\n\u001b[1;32m   1565\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstorage_options\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstorage_options\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1566\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1567\u001b[0;31m         self._reader = self._engines[engine](\n\u001b[0m\u001b[1;32m   1568\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_io\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1569\u001b[0m             \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/excel/_openpyxl.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, filepath_or_buffer, storage_options, engine_kwargs)\u001b[0m\n\u001b[1;32m    551\u001b[0m         \"\"\"\n\u001b[1;32m    552\u001b[0m         \u001b[0mimport_optional_dependency\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"openpyxl\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 553\u001b[0;31m         super().__init__(\n\u001b[0m\u001b[1;32m    554\u001b[0m             \u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    555\u001b[0m             \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, filepath_or_buffer, storage_options, engine_kwargs)\u001b[0m\n\u001b[1;32m    561\u001b[0m         )\n\u001b[1;32m    562\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mExcelFile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_workbook_class\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 563\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m    564\u001b[0m                 \u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_text\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    565\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    880\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    881\u001b[0m             \u001b[0;31m# Binary mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 882\u001b[0;31m             \u001b[0mhandle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    883\u001b[0m         \u001b[0mhandles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/Regional_Peak___Energy__2018_2023_ &2030.xlsx'"
          ]
        }
      ],
      "source": [
        "# forecast_2030_vs_cea_fixed.py\n",
        "# Forecast 2030 hourly regional load using trained Random Forest models\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.multioutput import MultiOutputRegressor\n",
        "from numpy.fft import rfft\n",
        "from datetime import datetime\n",
        "import calendar\n",
        "\n",
        "# --------------------------------------\n",
        "# Load annual peak + energy\n",
        "# --------------------------------------\n",
        "annual_file = \"/content/Regional_Peak___Energy__2018_2023_ &2030.xlsx\"\n",
        "ann_df = pd.read_excel(annual_file, engine=\"openpyxl\")\n",
        "ann_df.columns = [c.strip().lower() for c in ann_df.columns]\n",
        "\n",
        "# Rename columns\n",
        "ann_df = ann_df.rename(columns={\n",
        "    \"year\": \"Year\",\n",
        "    \"region\": \"Region\",\n",
        "    \"peak_mw\": \"year_peak\",\n",
        "    \"energy_mu\": \"year_energy\"\n",
        "})\n",
        "\n",
        "ann_2030 = ann_df[ann_df[\"Year\"] == 2030].copy()\n",
        "\n",
        "# --------------------------------------\n",
        "# Create dummy 2030 input dataframe\n",
        "# --------------------------------------\n",
        "region_cols = ann_2030[\"Region\"].tolist()\n",
        "df_2030_all = {}\n",
        "\n",
        "for reg in region_cols:\n",
        "    print(f\"Preparing inputs for: {reg}\")\n",
        "    year = 2030\n",
        "    n_hours = 366 * 24 if calendar.isleap(year) else 365 * 24\n",
        "    ts = pd.date_range(f\"{year}-01-01\", periods=n_hours, freq=\"h\")\n",
        "    df = pd.DataFrame(index=ts)\n",
        "    df[\"hour\"] = df.index.hour\n",
        "    df[\"dow\"] = df.index.dayofweek\n",
        "    df[\"month\"] = df.index.month\n",
        "    df[\"wknd\"] = (df[\"dow\"] >= 5).astype(int)\n",
        "    df[\"Year\"] = year\n",
        "    df[\"year_peak\"] = ann_2030.loc[ann_2030[\"Region\"] == reg, \"year_peak\"].values[0]\n",
        "    df[\"year_energy\"] = ann_2030.loc[ann_2030[\"Region\"] == reg, \"year_energy\"].values[0]\n",
        "    df_2030_all[reg] = df\n",
        "\n",
        "# --------------------------------------\n",
        "# Load & train model using 2018-2022\n",
        "# --------------------------------------\n",
        "train_file = \"/content/Hourly_Load_Data_2018_2022_LeapFixed.xlsx\"\n",
        "train_xl = pd.ExcelFile(train_file)\n",
        "train_raw = pd.concat([\n",
        "    train_xl.parse(str(y)).assign(Year=y) for y in range(2018, 2023)\n",
        "])\n",
        "\n",
        "# FFT utilities\n",
        "def fft_feature_array(x, n=3):\n",
        "    fft_vals = np.abs(rfft(x - np.mean(x)))[:n+1]\n",
        "    fft_vals = fft_vals / (np.linalg.norm(fft_vals) + 1e-6)\n",
        "    return fft_vals[1:n+1]\n",
        "\n",
        "def make_features(df):\n",
        "    feats = pd.DataFrame(index=df.index)\n",
        "    for h in [1, 2, 3, 6, 12, 24, 48, 168]:\n",
        "        feats[f\"lag_{h}\"] = df[\"Load\"].shift(h)\n",
        "    feats[\"peak_yday\"] = df[\"Load\"].rolling(24).max().shift(1)\n",
        "    feats[\"energy_yday\"] = df[\"Load\"].rolling(24).sum().shift(1)\n",
        "    feats[\"hour\"] = df.index.hour\n",
        "    feats[\"dow\"] = df.index.dayofweek\n",
        "    feats[\"month\"] = df.index.month\n",
        "    feats[\"wknd\"] = (feats[\"dow\"] >= 5).astype(int)\n",
        "    feats[\"year_peak\"] = df[\"year_peak\"]\n",
        "    feats[\"year_energy\"] = df[\"year_energy\"]\n",
        "    feats[\"mean_24\"] = df[\"Load\"].rolling(24).mean().shift(1)\n",
        "    feats[\"std_24\"] = df[\"Load\"].rolling(24).std().shift(1)\n",
        "    for i in range(1, 4):\n",
        "        feats[f\"fft_{i}\"] = df[\"Load\"].rolling(24).apply(\n",
        "            lambda x: fft_feature_array(x)[i-1] if np.isfinite(x).sum() == 24 else np.nan,\n",
        "            raw=True\n",
        "        ).shift(1)\n",
        "    feats[\"target\"] = df[\"Load\"]\n",
        "    return feats.dropna()\n",
        "\n",
        "results = []\n",
        "\n",
        "for reg in region_cols:\n",
        "    print(f\"\\n=== Predicting 2030 Load for: {reg} ===\")\n",
        "    train_raw[\"Load\"] = train_raw[reg]\n",
        "    ann_years = ann_df.drop_duplicates(\"Year\").set_index(\"Year\")\n",
        "    train_raw[\"year_peak\"] = train_raw[\"Year\"].map(ann_years[\"year_peak\"])\n",
        "    train_raw[\"year_energy\"] = train_raw[\"Year\"].map(ann_years[\"year_energy\"])\n",
        "    feat_train = make_features(train_raw)\n",
        "\n",
        "    X_train = feat_train.drop(columns=[\"target\", \"mean_24\", \"std_24\", \"fft_1\", \"fft_2\", \"fft_3\"])\n",
        "    Y_train = feat_train[[\"target\", \"mean_24\", \"std_24\", \"fft_1\", \"fft_2\", \"fft_3\"]]\n",
        "\n",
        "    model = MultiOutputRegressor(RandomForestRegressor(\n",
        "        n_estimators=200, max_depth=16, min_samples_split=10,\n",
        "        max_features=\"sqrt\", n_jobs=-1, random_state=42\n",
        "    ))\n",
        "    model.fit(X_train, Y_train)\n",
        "\n",
        "    # Inference 2030\n",
        "    X_2030 = df_2030_all[reg]\n",
        "    Y_2030_hat = pd.DataFrame(model.predict(X_2030),\n",
        "                              columns=Y_train.columns,\n",
        "                              index=X_2030.index)\n",
        "\n",
        "    pred_peak = Y_2030_hat[\"target\"].max()\n",
        "    pred_energy = Y_2030_hat[\"target\"].sum() / 1000  # MW -> MU\n",
        "\n",
        "    results.append([\n",
        "        reg,\n",
        "        pred_peak,\n",
        "        ann_2030.loc[ann_2030[\"Region\"] == reg, \"year_peak\"].values[0],\n",
        "        pred_energy,\n",
        "        ann_2030.loc[ann_2030[\"Region\"] == reg, \"year_energy\"].values[0]\n",
        "    ])\n",
        "\n",
        "# --------------------------------------\n",
        "# Show Results\n",
        "# --------------------------------------\n",
        "df_comp = pd.DataFrame(results, columns=[\"Region\", \"Pred_Peak\", \"CEA_Peak\", \"Pred_Energy\", \"CEA_Energy\"])\n",
        "df_comp.set_index(\"Region\", inplace=True)\n",
        "df_comp[\"%Error_Peak\"] = 100 * (df_comp[\"Pred_Peak\"] - df_comp[\"CEA_Peak\"]) / df_comp[\"CEA_Peak\"]\n",
        "df_comp[\"%Error_Energy\"] = 100 * (df_comp[\"Pred_Energy\"] - df_comp[\"CEA_Energy\"]) / df_comp[\"CEA_Energy\"]\n",
        "\n",
        "print(\"\\n--- 2030 Forecast vs CEA Targets ---\")\n",
        "print(df_comp.round(2))\n",
        "df_comp.to_csv(\"forecast_vs_cea_2030.csv\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4."
      ],
      "metadata": {
        "id": "Tg8yc34abwP4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "i9doVLndQHpD",
        "outputId": "29142434-3621-4b3a-ec16-35e9e3952aeb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Forecasting 2030 for Northern Region ===\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-1767314075.py:111: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
            "  df_full = pd.concat([df_seed, df_pred])\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Forecasting 2030 for North Eastern Region ===\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-1767314075.py:111: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
            "  df_full = pd.concat([df_seed, df_pred])\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Forecasting 2030 for Eastern Region ===\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-1767314075.py:111: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
            "  df_full = pd.concat([df_seed, df_pred])\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Forecasting 2030 for Southern Region ===\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-1767314075.py:111: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
            "  df_full = pd.concat([df_seed, df_pred])\n"
          ]
        }
      ],
      "source": [
        "# forecast_2030_from_excel.py\n",
        "# Forecast 2030 hourly regional load using Excel input instead of text input\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.multioutput import MultiOutputRegressor\n",
        "from numpy.fft import rfft\n",
        "import calendar\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 1. Load Excel input for 2018â€“2022\n",
        "# ------------------------------------------------------------\n",
        "train_file = \"/content/Hourly_Load_Data_2018_2022_LeapFixed.xlsx\"\n",
        "train_xl = pd.ExcelFile(train_file)\n",
        "\n",
        "train_raw = pd.concat([\n",
        "    train_xl.parse(str(y)).assign(Year=y)\n",
        "    for y in range(2018, 2023)\n",
        "], ignore_index=True)\n",
        "\n",
        "full_index = []\n",
        "for yr in range(2018, 2023):\n",
        "    n_hours = 366 * 24 if calendar.isleap(yr) else 365 * 24\n",
        "    full_index.extend(pd.date_range(f\"{yr}-01-01\", periods=n_hours, freq=\"h\"))\n",
        "train_raw.index = full_index[:len(train_raw)]\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 2. 2030 CEA target values\n",
        "# ------------------------------------------------------------\n",
        "regions = ['Northern Region', 'North Eastern Region', 'Eastern Region',\n",
        "           'Southern Region', 'Western Region']\n",
        "\n",
        "cea_2030 = {\n",
        "    \"Northern Region\":        {\"peak\": 127553, \"energy\": 773545},\n",
        "    \"Western Region\":         {\"peak\": 114766, \"energy\": 763198},\n",
        "    \"Southern Region\":        {\"peak\": 107259, \"energy\": 596557},\n",
        "    \"Eastern Region\":         {\"peak\": 50420,  \"energy\": 308103},\n",
        "    \"North Eastern Region\":   {\"peak\": 6519,   \"energy\": 32373}\n",
        "}\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 3. FFT + Feature Builder\n",
        "# ------------------------------------------------------------\n",
        "def fft_feature_array(x, n=3):\n",
        "    fft_vals = np.abs(rfft(x - np.mean(x)))[:n+1]\n",
        "    fft_vals = fft_vals / (np.linalg.norm(fft_vals) + 1e-6)\n",
        "    return fft_vals[1:n+1]\n",
        "\n",
        "def make_features(df, load_col):\n",
        "    feats = pd.DataFrame(index=df.index)\n",
        "    for h in [1, 2, 3, 6, 12, 24, 48, 168]:\n",
        "        feats[f\"lag_{h}\"] = df[load_col].shift(h)\n",
        "    feats[\"peak_yday\"] = df[load_col].rolling(24).max().shift(1)\n",
        "    feats[\"energy_yday\"] = df[load_col].rolling(24).sum().shift(1)\n",
        "    feats[\"hour\"] = df.index.hour\n",
        "    feats[\"dow\"] = df.index.dayofweek\n",
        "    feats[\"month\"] = df.index.month\n",
        "    feats[\"wknd\"] = (feats[\"dow\"] >= 5).astype(int)\n",
        "    feats[\"year_peak\"] = df[\"year_peak\"]\n",
        "    feats[\"year_energy\"] = df[\"year_energy\"]\n",
        "    feats[\"mean_24\"] = df[load_col].rolling(24).mean().shift(1)\n",
        "    feats[\"std_24\"] = df[load_col].rolling(24).std().shift(1)\n",
        "    for i in range(1, 4):\n",
        "        feats[f\"fft_{i}\"] = df[load_col].rolling(24).apply(\n",
        "            lambda x: fft_feature_array(x)[i-1] if np.isfinite(x).sum()==24 else np.nan,\n",
        "            raw=True\n",
        "        ).shift(1)\n",
        "    feats[\"target\"] = df[load_col]\n",
        "    # Do not dropna here, handle NaNs during prediction\n",
        "    return feats\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 4. Train + Forecast (Iterative)\n",
        "# ------------------------------------------------------------\n",
        "def forecast_region_2030(reg):\n",
        "    train_raw[\"Load\"] = train_raw[reg]\n",
        "    train_raw[\"year_peak\"] = cea_2030[reg][\"peak\"]\n",
        "    train_raw[\"year_energy\"] = cea_2030[reg][\"energy\"] * 1000\n",
        "    feat_train = make_features(train_raw, \"Load\").dropna() # Drop NaNs for training\n",
        "\n",
        "    X_train = feat_train.drop(columns=[\"target\", \"mean_24\", \"std_24\", \"fft_1\", \"fft_2\", \"fft_3\"])\n",
        "    Y_train = feat_train[[\"target\", \"mean_24\", \"std_24\", \"fft_1\", \"fft_2\", \"fft_3\"]]\n",
        "\n",
        "    model = MultiOutputRegressor(\n",
        "        RandomForestRegressor(\n",
        "            n_estimators=200, max_depth=16, min_samples_split=10,\n",
        "            max_features=\"sqrt\", n_jobs=-1, random_state=42\n",
        "        )\n",
        "    )\n",
        "    model.fit(X_train, Y_train)\n",
        "\n",
        "    year = 2030\n",
        "    n_hours = 366 * 24 if calendar.isleap(year) else 365 * 24\n",
        "    ts_2030 = pd.date_range(f\"{year}-01-01\", periods=n_hours, freq=\"h\")\n",
        "    df_pred = pd.DataFrame(index=ts_2030, columns=[\"Load\"])\n",
        "    df_pred[\"hour\"] = df_pred.index.hour\n",
        "    df_pred[\"dow\"] = df_pred.index.dayofweek\n",
        "    df_pred[\"month\"] = df_pred.index.month\n",
        "    df_pred[\"wknd\"] = (df_pred[\"dow\"] >= 5).astype(int)\n",
        "    df_pred[\"year_peak\"] = cea_2030[reg][\"peak\"]\n",
        "    df_pred[\"year_energy\"] = cea_2030[reg][\"energy\"] * 1000\n",
        "\n",
        "    historical_window_size = 168 # Max lag/rolling window size\n",
        "    df_seed = train_raw[[\"Load\", \"year_peak\", \"year_energy\"]].iloc[-historical_window_size:].copy()\n",
        "    df_seed[\"hour\"] = df_seed.index.hour\n",
        "    df_seed[\"dow\"] = df_seed.index.dayofweek\n",
        "    df_seed[\"month\"] = df_seed.index.month\n",
        "    df_seed[\"wknd\"] = (df_seed[\"dow\"] >= 5).astype(int)\n",
        "\n",
        "    df_full = pd.concat([df_seed, df_pred])\n",
        "    df_full[\"Load\"] = df_full[\"Load\"].astype(float)\n",
        "\n",
        "    for t in ts_2030:\n",
        "        # Use a window that includes the necessary historical data for feature calculation\n",
        "        window = df_full.loc[:t].iloc[-historical_window_size-1:]\n",
        "        feats = make_features(window, \"Load\")\n",
        "\n",
        "        # Check if features were successfully created for the current timestamp\n",
        "        if t in feats.index:\n",
        "            X_t = feats.loc[[t]].drop(columns=[\"target\", \"mean_24\", \"std_24\", \"fft_1\", \"fft_2\", \"fft_3\"])\n",
        "            y_hat = model.predict(X_t)[0]\n",
        "            df_full.at[t, \"Load\"] = y_hat[0] # Update df_full with predicted load\n",
        "        else:\n",
        "            # Handle cases where features cannot be calculated (e.g., early hours of forecast)\n",
        "            # For simplicity, we can carry over the last known load or use a default\n",
        "            # Here, we'll just note it. More sophisticated imputation might be needed.\n",
        "            print(f\"Could not calculate features for {t}. Using previous hour's load.\")\n",
        "            df_full.at[t, \"Load\"] = df_full.loc[df_full.index < t, \"Load\"].iloc[-1]\n",
        "\n",
        "\n",
        "    return df_full.loc[ts_2030, \"Load\"] # Return only the 2030 predictions\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 5. Forecast all regions + compare\n",
        "# ------------------------------------------------------------\n",
        "results = []\n",
        "for reg in regions:\n",
        "    print(f\"\\n=== Forecasting 2030 for {reg} ===\")\n",
        "    y_hat = forecast_region_2030(reg)\n",
        "    pred_peak = y_hat.max()\n",
        "    pred_energy = y_hat.sum() / 1000\n",
        "    results.append([\n",
        "        reg,\n",
        "        pred_peak,\n",
        "        cea_2030[reg][\"peak\"],\n",
        "        pred_energy,\n",
        "        cea_2030[reg][\"energy\"]\n",
        "    ])\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 6. Results Table\n",
        "# ------------------------------------------------------------\n",
        "df_comp = pd.DataFrame(results, columns=[\n",
        "    \"Region\", \"Pred_Peak\", \"CEA_Peak\", \"Pred_Energy\", \"CEA_Energy\"])\n",
        "df_comp.set_index(\"Region\", inplace=True)\n",
        "df_comp[\"%Error_Peak\"] = 100 * (df_comp[\"Pred_Peak\"] - df_comp[\"CEA_Peak\"]) / df_comp[\"CEA_Peak\"]\n",
        "df_comp[\"%Error_Energy\"] = 100 * (df_comp[\"Pred_Energy\"] - df_comp[\"CEA_Energy\"]) / df_comp[\"CEA_Energy\"]\n",
        "\n",
        "print(\"\\n--- 2030 Forecast vs CEA Targets ---\")\n",
        "print(df_comp.round(2))\n",
        "df_comp.to_csv(\"forecast_vs_cea_2030_from_excel_iterative.csv\")"
      ]
    }
  ]
}